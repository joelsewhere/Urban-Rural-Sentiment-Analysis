{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "What are the values, interests, personalities, opinions, language of Chicago and the rest of Illinois? And do they differ?\n",
    "\n",
    "In this notebook, using a dataset of 1.6 million tweets classified with either a positive or negative sentiment––I  train a neural networks to predict the sentiment of a tweet with about 78% accuracy. I then run the neural network over 70,000 tweets from Illinois and examine how users from Chicago and the rest of Illinois feel about a given topic. I then export the word vectorizations and predicted sentiment and load them into a dash application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/joel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to /Users/joel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import string\n",
    "import nltk\n",
    "import keras\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Machine Learning:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Packages for data preparation\n",
    "from process_dataframe import process_dataframe\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing import text, sequence\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Turn off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean(text, split = True):\n",
    "\n",
    "           \n",
    "    digits = string.digits\n",
    "    punctuation = string.punctuation\n",
    "    punctuation += '’'\n",
    "    \n",
    "#     emoji_pattern = re.compile(\"[\"\n",
    "#             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#                                \"]+\", flags=re.UNICODE)\n",
    "#     no_emoji = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    \n",
    "    no_emoji  = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    d_trump = no_emoji.replace('realdonaldtrump', 'donald trump')\n",
    "    no_stops  = remove_stops(d_trump)\n",
    "    no_http = remove_http(no_stops)\n",
    "    no_digits = no_http.translate(str.maketrans('', '', digits))\n",
    "    cleaned = no_digits.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    if split:\n",
    "        split = cleaned.split()\n",
    "        return split\n",
    "    else: \n",
    "        return cleaned \n",
    "    \n",
    "def remove_http(text):\n",
    "    split_text = text.split()\n",
    "    for i in split_text:\n",
    "        if 'http' in i:\n",
    "            text = text.replace(i, '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "    \n",
    "def remove_stops(text):\n",
    "    stops = stopwords.words('english')\n",
    "    stops += ['i’m', 'it’s', \n",
    "              'don’t', 'can’t', \n",
    "              'that’s', '2', \n",
    "              'lol', 'i',\n",
    "             'get', 'the','i’m', '-',\n",
    "             \"i'm\", 'it', 'a', 'u', 'amp', '•', 'i’ve',\n",
    "             'chicago', 'tictac', 'flippen', 'click', 'link', 'bio', 'job',\n",
    "             '_', 'apply', 'jobs', 'hiring', 'openings', 'opening', 'could', 'might',\n",
    "             'via']\n",
    "\n",
    "    split = text.split()\n",
    "    removed = [x.lower() for x in split if x.lower() not in stops]\n",
    "    \n",
    "    return ' '.join(removed)\n",
    "\n",
    "\n",
    "def tokenize(self, vocab_size = 20000, maxlen = 50):\n",
    "    vocabulary_size = vocab_size\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "    tokenizer.fit_on_texts(self.stemmed)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(self.stemmed)\n",
    "    data = pad_sequences(sequences_1, maxlen=maxlen)\n",
    "    return data\n",
    "\n",
    "\n",
    "def bigrams(bag):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    finder = BigramCollocationFinder.from_words(bag)\n",
    "    finder.apply_freq_filter(10)\n",
    "\n",
    "    scored = finder.score_ngrams(bigram_measures.likelihood_ratio)\n",
    "    return scored\n",
    "\n",
    "def trigrams(bag):\n",
    "    trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "    finder = TrigramCollocationFinder.from_words(bag)\n",
    "    scored = finder.score_ngrams(trigram_measures.likelihood_ratio)\n",
    "    return scored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction\n",
    "\n",
    "1. Using a [twitter sentiment dataset](https://www.kaggle.com/kazanova/sentiment140) from kaggle, I will try to create a model that can predict the sentiment of tweets.\n",
    "2. Then I will run the model over my twitter data from Illinois\n",
    "3. Compare the sentiments of Chicago and Illinois."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Sentiment Data: \n",
    "\n",
    "Now let's see if I can create a model to predict the sentiment of tweets. For simplicity sake, I will run the new dataset through a class that filters the data through the same cleaning process used above.\n",
    "\n",
    "*Note:* For simplicity's sake, I will be running this dataset through a class called process_dataframe that replicates the cleaning process done above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'latin', names = ['target', 'ids', \n",
    "                                                                                             'date', 'flag', \n",
    "                                                                                             'user', 'text'])\n",
    "\n",
    "sentiment_process = process_dataframe(df, 'text')\n",
    "\n",
    "\n",
    "df.text = sentiment_process.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "change = {4:1,\n",
    "         0:0}\n",
    "df.target = df.target.map(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [switchfoot, awww, thats, bummer, shoulda, dav...\n",
       "1    [upset, cant, update, facebook, texting, it, c...\n",
       "2    [kenichan, dived, many, times, ball, managed, ...\n",
       "3                     [whole, body, feels, itchy, fir]\n",
       "4    [nationwideclass, no, behaving, all, mad, here...\n",
       "5                              [kwesidei, whole, crew]\n",
       "6                                          [need, hug]\n",
       "7    [loltrish, hey, long, time, see, yes, rains, b...\n",
       "8                                      [tatianak, nop]\n",
       "9                                    [twittera, muera]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'll tokenize the words so they can be fed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df.text)\n",
    "sequences_1 = tokenizer.texts_to_sequences(df.text)\n",
    "data = pad_sequences(sequences_1, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below to save the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # saving\n",
    "# with open('tokenizer.pickle', 'wb') as handle:\n",
    "#     pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print out the shape of the matrix so we know how to calibrate the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now feed the text data into a Gated Recurrent Unit Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, GRU\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell: \n",
    "1. Create a train test for evaluating the accuracy of the neural network.\n",
    "2. Compile the layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain_, ytest_ = train_test_split(data, df.target, train_size = .99)\n",
    "\n",
    "ytrain = to_categorical(ytrain_)\n",
    "ytest = to_categorical(ytest_)\n",
    "\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Embedding(20000, 128))\n",
    "gru_model.add(GRU(50, return_sequences=True))\n",
    "gru_model.add(GlobalMaxPool1D())\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(50, activation='relu'))\n",
    "gru_model.add(Dropout(0.5))\n",
    "gru_model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coolio. Let's take a look at the layers and make sure evrything looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_45 (Embedding)     (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 50)          26850     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_39 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_235 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_78 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,589,502\n",
      "Trainable params: 2,589,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIT TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1425600 samples, validate on 158400 samples\n",
      "Epoch 1/2\n",
      "1425600/1425600 [==============================] - 3610s 3ms/step - loss: 0.4846 - acc: 0.7699 - val_loss: 0.4528 - val_acc: 0.7866\n",
      "Epoch 2/2\n",
      "1425600/1425600 [==============================] - 3819s 3ms/step - loss: 0.4574 - acc: 0.7878 - val_loss: 0.4498 - val_acc: 0.7890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b43d5bc18>"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.fit(xtrain, ytrain, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THREE HOURS LATER:** Let's take a look at how accurately the neural network can predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1584000/1584000 [==============================] - 701s 443us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4286012996438176, 0.8019842171717172]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.evaluate(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 7s 443us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4539098492860794, 0.78325]"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gru_model.evaluate(xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! 78% accuracy. This obviously is not perfect, but sentiment an abstract thing to measure. Let's what this looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "For sentiment analysis I will:\n",
    "1. Run the GRU sentiment model over illinois data and create a sentiment column for those predicted values.\n",
    "2. Create seperate dataframes for Chicago and Illinois data\n",
    "3.  Create a Word2Vec model for Chicago and Illinois to find words that are in relationship with a given topic.\n",
    "\n",
    "The dataframe containing predicted sentiment and the word2vec models in this section are the primary pieces of data at work in a Dash app made to vizualize the differencing between the two groups.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from process_dataframe import process_dataframe\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from process_dataframe import process_dataframe\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "illinois = pd.read_csv('final_uncleaned.csv')\n",
    "\n",
    "illinois = illinois[illinois.target.notnull()]\n",
    "\n",
    "\n",
    "illinois = illinois[illinois.text.notnull()]\n",
    "illinois.reset_index(drop =  True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "lincoln = process_dataframe(illinois, 'text')\n",
    "\n",
    "illinois['text'] = lincoln.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tokenizer = tokenizer.texts_to_sequences(lincoln.data)\n",
    "pred_data = sequence.pad_sequences(pred_tokenizer, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the GRU neural network over the tweets from Illinois and create predicted sentiment of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    46451\n",
       "0    23548\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = gru_model.predict_classes(pred_data)\n",
    "\n",
    "illinois['sentiment'] = preds\n",
    "\n",
    "illinois.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, 1 repsents a positive sentiment and 2 a negative sentiment. \n",
    "\n",
    "Aw what a bunch of positive folks :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = []\n",
    "for i in illinois.index:\n",
    "    collected = [x for x in illinois.text[i] if len(x) < 15]\n",
    "    column.append(collected)\n",
    "\n",
    "illinois.text = column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text data is currenty a list type. Let's change it to a string type so it can be vectorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "illinois.text = illinois.text.apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Word2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, I will create three word2vec models.\n",
    "1. All illinois data––*What are topical relationships of the state as a whole?*\n",
    "2. Chicago data––*What is Chicago talking about?*\n",
    "3. All of Illinois excluding Chicago––*What differences can be found between IL and Chicago?*\n",
    "\n",
    "**Then** I will save each of the word embedding models, and vizualize them with a dash app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_x = illinois[illinois.target == 0]\n",
    "chi = illinois[illinois.target == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Illinois –– All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4863291, 4905810)"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = Word2Vec(illinois.text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "w2v.train(illinois.text, total_examples=w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the most similar words! (I love this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('airport', 0.9049680233001709),\n",
       " ('east', 0.8951677083969116),\n",
       " ('kishwaukee', 0.8946200013160706),\n",
       " ('central', 0.8909432888031006),\n",
       " ('north', 0.873832643032074),\n",
       " ('daytripper', 0.8731734752655029),\n",
       " ('loop', 0.8699049353599548),\n",
       " ('southern', 0.8686106204986572),\n",
       " ('state', 0.8685200214385986),\n",
       " ('training', 0.8682582378387451)]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive = ['illinois'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright let's repeat the process for Chicago and Illinois excluding Chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2382453, 2403220)"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_chi = Word2Vec(illinois.text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "w2v_chi.train(chi.text, total_examples=w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('address', 0.9894412755966187),\n",
       " ('allow', 0.9891008138656616),\n",
       " ('politics', 0.9889135956764221),\n",
       " ('issue', 0.9869572520256042),\n",
       " ('responsible', 0.9869493842124939),\n",
       " ('lack', 0.9862771034240723),\n",
       " ('candidates', 0.9845634698867798),\n",
       " ('totally', 0.9839571714401245),\n",
       " ('destroy', 0.9835915565490723),\n",
       " ('prove', 0.9832010269165039)]"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_chi.wv.most_similar(positive = ['government'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illinois excluding Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2481015, 2502590)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_il_x = Word2Vec(illinois.text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "w2v_il_x.train(il_x.text, total_examples=w2v.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('politicians', 0.9770750999450684),\n",
       " ('decency', 0.9761868715286255),\n",
       " ('lies', 0.9746777415275574),\n",
       " ('rights', 0.9728584289550781),\n",
       " ('protect', 0.9712259769439697),\n",
       " ('privatesector', 0.9706145524978638),\n",
       " ('americans', 0.9696345329284668),\n",
       " ('allow', 0.9693350791931152),\n",
       " ('healthcare', 0.9692049622535706),\n",
       " ('recession', 0.9686658382415771)]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_il_x.wv.most_similar(positive = ['government'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the cell below the save the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v.save('illinois_vectors.bin')\n",
    "#w2v_chi.save('chicago_vectors.bin')\n",
    "#w2v_il_x.save('il_x_vectors.bin')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
